{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Part 1: Dataset Simulation",
   "id": "fec51b21a2c36160"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import random\n",
    "from google import genai\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import re"
   ],
   "id": "69d3114fc3a0c9b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# CONFIGURATION\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "client = genai.Client(api_key=API_KEY)"
   ],
   "id": "455f0bab46fd1ea9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# DEFINE THE GENAI FUNCTION for TEXT GENERATION\n",
    "def generate_customer_feedback(persona, sentiment, model=\"gemini-2.0-flash\", num_sentences=200):\n",
    "    \"\"\"\n",
    "    Sends a prompt to Gemini to generate realistic text and returns a list of strings.\n",
    "    This function is defensive: it will try multiple parsing strategies and always\n",
    "    return a list of length `num_sentences` (padding/repeating if necessary).\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are simulating a customer database for a bank.\n",
    "    Generate exactly {num_sentences} unique, realistic customer feedback sentences (10-20 words each).\n",
    "\n",
    "    Customer Profile: {persona}\n",
    "    Sentiment:{sentiment}\n",
    "\n",
    "    - If 'Young Adult': Mention apps, crypto, student loans, splitting bills, fast cash. Use casual language.\n",
    "    - If 'Professional': Mention interest rates, mortgages, wire transfers, business accounts. Use formal language.\n",
    "    - If 'Retiree': Mention branches, pensions, passwords, phone support, safety. Use polite but confused language.\n",
    "\n",
    "    - If Sentiment is 'Positive': Praise speed, features, or service.\n",
    "    - If Sentiment is 'Negative': Complain about fees, crashes, delays, or rejections.\n",
    "\n",
    "    Output format: A raw JSON list of strings. Example: [\"text1\", \"text2\"]\n",
    "    Do not add Markdown formatting or extra text.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=[prompt]\n",
    "        )\n",
    "\n",
    "        # Try multiple ways to get text from the response object\n",
    "        raw_text = None\n",
    "        if hasattr(response, 'text') and isinstance(response.text, str):\n",
    "            raw_text = response.text\n",
    "        else:\n",
    "            # Fallback to stringifying the response\n",
    "            raw_text = str(response)\n",
    "\n",
    "        if raw_text is None:\n",
    "            raise ValueError(\"No text found in model response\")\n",
    "\n",
    "        # Remove common code block markers\n",
    "        cleaned = raw_text.strip().replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "\n",
    "        # Search for the first JSON array in the response (robust against extra text)\n",
    "        # Use non-greedy match so we don't accidentally capture across multiple arrays\n",
    "        match = re.search(r\"\\[.*?\\]\", cleaned, re.S)\n",
    "        json_text = match.group(0) if match else cleaned\n",
    "\n",
    "        parsed = json.loads(json_text)\n",
    "\n",
    "        # Normalize to list of non-empty strings\n",
    "        if isinstance(parsed, list):\n",
    "            texts = [str(x).strip() for x in parsed if isinstance(x, str) and str(x).strip()]\n",
    "        else:\n",
    "            # If model returned a dict or other structure, try to find strings inside\n",
    "            texts = []\n",
    "            def _extract_strings(obj):\n",
    "                if isinstance(obj, str):\n",
    "                    return [obj]\n",
    "                if isinstance(obj, dict):\n",
    "                    out = []\n",
    "                    for v in obj.values():\n",
    "                        out.extend(_extract_strings(v))\n",
    "                    return out\n",
    "                if isinstance(obj, list):\n",
    "                    out = []\n",
    "                    for v in obj:\n",
    "                        out.extend(_extract_strings(v))\n",
    "                    return out\n",
    "                return []\n",
    "            texts = [s.strip() for s in _extract_strings(parsed) if s and s.strip()]\n",
    "\n",
    "        # If we still have nothing, raise to hit the exception handler below\n",
    "        if not texts:\n",
    "            raise ValueError(\"Parsed response contained no usable strings\")\n",
    "\n",
    "        # Ensure exactly num_sentences are returned: if fewer, repeat/shuffle to pad\n",
    "        if len(texts) < num_sentences:\n",
    "            # If there are some unique texts, repeat them to reach requested count\n",
    "            orig = texts.copy()\n",
    "            i = 0\n",
    "            while len(texts) < num_sentences and orig:\n",
    "                texts.append(orig[i % len(orig)])\n",
    "                i += 1\n",
    "        elif len(texts) > num_sentences:\n",
    "            texts = texts[:num_sentences]\n",
    "\n",
    "        return texts\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error fetching/parsing from Gemini: {e}\")\n",
    "        # Return a deterministic fallback list of the requested length\n",
    "        return [\"Error generating text.\" for _ in range(num_sentences)]\n",
    "\n",
    "\n",
    "data_cache = {\n",
    "        \"young_adult\": {\"positive\": [], \"negative\": []},\n",
    "        \"professional\": {\"positive\": [], \"negative\": []},\n",
    "        \"retiree\": {\"positive\": [], \"negative\": []}\n",
    "    }\n",
    "\n",
    "# --- Young Adult ---\n",
    "ya_positive = generate_customer_feedback(\"Young Adult\", \"Positive\")\n",
    "ya_negative = generate_customer_feedback(\"Young Adult\", \"Negative\")\n",
    "\n",
    "# --- Professional ---\n",
    "pro_positive = generate_customer_feedback(\"Professional\", \"Positive\")\n",
    "pro_negative = generate_customer_feedback(\"Professional\", \"Negative\")\n",
    "\n",
    "# --- Retiree ---\n",
    "ret_positive = generate_customer_feedback(\"Retiree\", \"Positive\")\n",
    "ret_negative = generate_customer_feedback(\"Retiree\", \"Negative\")"
   ],
   "id": "7be0ab23ee266293"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_feedback_text(persona, is_default):\n",
    "    \"\"\"\n",
    "    Selects a random feedback string based on Persona and Default Status.\n",
    "    \"\"\"\n",
    "    # 1. Determine Base Sentiment\n",
    "    # Default = 1 means they are high risk/struggling -> likely Negative feedback\n",
    "    # Default = 0 means they are repaying -> likely Positive feedback\n",
    "    use_negative_sentiment = (is_default == 1)\n",
    "\n",
    "    # 2. Add \"Noise\"\n",
    "    if random.random() < 0.10:\n",
    "        use_negative_sentiment = not use_negative_sentiment\n",
    "\n",
    "    selected_list = []\n",
    "    \n",
    "    # 3. Select the appropriate list based on Persona and Sentiment\n",
    "    if persona == \"young_adult\":\n",
    "        selected_list = ya_negative if use_negative_sentiment else ya_positive\n",
    "    elif persona == \"professional\":\n",
    "        selected_list = pro_negative if use_negative_sentiment else pro_positive\n",
    "    elif persona == \"retiree\":\n",
    "        selected_list = ret_negative if use_negative_sentiment else ret_positive\n",
    "\n",
    "    # 4. Return a random sentence from selected list\n",
    "    if not selected_list:\n",
    "        return \"Service was okay.\"      \n",
    "    return random.choice(selected_list)"
   ],
   "id": "c4cae01379373146"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# SIMULATE DATASET \n",
    "fake = Faker()\n",
    "Faker.seed(42)\n",
    "\n",
    "def generate_data(num_records=1000):\n",
    "    data = []\n",
    "\n",
    "    for _ in range(num_records):\n",
    "        # --- Demographics ---\n",
    "        profile = fake.profile()\n",
    "        customer_id = f\"CUST-{fake.unique.random_int(min=100000, max=999999)}\"\n",
    "        gender = profile['sex']\n",
    "        age = random.randint(18, 80)\n",
    "\n",
    "\n",
    "        # Simulate 3 types customer persona: Young Adult, Professional, Retiree\n",
    "        if age < 25:\n",
    "            persona = \"young_adult\"\n",
    "            job = np.random.choice([\"Student\", \"Intern\", \"Part-time Server\", \"Barista\", \"None\"])\n",
    "            income = np.random.normal(15000, 5000)\n",
    "            spend_score = np.random.normal(70, 10) # High spend relative to income\n",
    "\n",
    "        elif 25 <= age < 60:\n",
    "            persona = \"professional\"\n",
    "            job = profile['job']\n",
    "            income = np.random.normal(85000, 25000)\n",
    "            spend_score = np.random.normal(50, 15)\n",
    "\n",
    "        else:\n",
    "            persona = \"retiree\"\n",
    "            job = \"retired\"\n",
    "            income = np.random.normal(45000, 10000)\n",
    "            spend_score = np.random.normal(30, 10) # Low spend\n",
    "\n",
    "\n",
    "        # --- Transactional Numerics ---\n",
    "        #  Credit score\n",
    "        if persona == \"professional\":\n",
    "            credit_score = int(np.random.normal(720, 50))\n",
    "        elif persona == \"young_adult\":\n",
    "            credit_score = int(np.random.normal(600, 80))\n",
    "        else:\n",
    "            credit_score = int(np.random.normal(680, 60))\n",
    "        \n",
    "        credit_score = max(300, min(850, credit_score)) #real world limits (300-850)\n",
    "\n",
    "        # Account balance\n",
    "        if persona == \"young_adult\":\n",
    "            savings_rate = np.random.normal(0.05, 0.02)\n",
    "            accumulation_factor = random.uniform(0.1, 1.0)\n",
    "\n",
    "        elif persona == \"professional\":\n",
    "            savings_rate = np.random.normal(0.20, 0.05)\n",
    "            accumulation_factor = random.uniform(1.0, 3.0)\n",
    "\n",
    "        else: # Retiree\n",
    "            savings_rate = np.random.normal(0.08, 0.04)\n",
    "            accumulation_factor = random.uniform(2.0, 5.0)\n",
    "\n",
    "        savings_rate = max(0.01, min(0.50, savings_rate))\n",
    "        account_balance = (income * savings_rate) * accumulation_factor * random.uniform(0.1, 1.5)\n",
    "        \n",
    "        # Loan Request Details\n",
    "        max_possible_loan = income * 0.6\n",
    "        loan_amount = round(random.uniform(2000, max_possible_loan), 2)\n",
    "        loan_term = random.choice([12, 24, 36, 60])\n",
    "\n",
    "        risk_probability = 0.1 # Base 10% risk\n",
    "        \n",
    "        # Rule 1: Low Credit Score increases risk significantly\n",
    "        if credit_score < 580: risk_probability += 0.40\n",
    "        elif credit_score < 650: risk_probability += 0.20\n",
    "        \n",
    "        # Rule 2: High Loan-to-Income ratio increases risk\n",
    "        dti_ratio = loan_amount / income\n",
    "        if dti_ratio > 0.4: risk_probability += 0.25\n",
    "        \n",
    "        # Rule 3: Additional risk for Young Adults\n",
    "        if persona == \"young_adult\": risk_probability += 0.05\n",
    "\n",
    "        # Cap probability at 0.95 \n",
    "        risk_probability = min(0.95, risk_probability)\n",
    "\n",
    "        is_default = 1 if random.random() < risk_probability else 0\n",
    "\n",
    "\n",
    "        # --- 4. Feedback Text Column ---\n",
    "        feedback_text = get_feedback_text(persona, is_default)\n",
    "        \n",
    "        record = {\n",
    "            \"Customer_ID\": customer_id,\n",
    "            \"Age\": age,\n",
    "            \"Gender\": gender,\n",
    "            \"Persona\": persona,\n",
    "            \"Job\": job,\n",
    "            \"Annual_Income\": round(income, 2),\n",
    "            \"Credit_Score\": credit_score,\n",
    "            \"Account_Balance\": round(account_balance, 2),\n",
    "            \"Spending_Score\": round(spend_score, 2),\n",
    "            \"Loan_Amount\": loan_amount,\n",
    "            \"Loan_Term_Months\": loan_term,\n",
    "            \"Loan_Default\": is_default, #\n",
    "            \"Customer_Feedback\": feedback_text\n",
    "        }\n",
    "        data.append(record)\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Generate and View\n",
    "df_customers = generate_data(1000)\n",
    "print(df_customers.head())"
   ],
   "id": "2aaac83f599b1230"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save to CSV\n",
    "df_customers.to_csv('financial_data_simulated.csv', index=False)\n",
    "print(f\"Dataset saved to 'financial_data_simulated.csv'\")"
   ],
   "id": "d3128ce07b807000"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Part 2: Feature Engineering and Data Cleaning (refined)",
   "id": "e2851ccd3c285d47"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Run this cell to use the CSV if you prefer not to regenerate the dataset in Part 1\n",
    "if os.path.exists('financial_data_simulated.csv'):\n",
    "    try:\n",
    "        df_customers = pd.read_csv('financial_data_simulated.csv')\n",
    "        print('Loaded financial_data_simulated.csv with shape', df_customers.shape)\n",
    "    except Exception as e:\n",
    "        print('Could not load CSV, using in-memory df_customers if present:', e)\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "# Small lexicons for quick polarity heuristics\n",
    "POS_WORDS = {\"good\", \"great\", \"fast\", \"love\", \"excellent\", \"happy\", \"satisfied\", \"helpful\", \"smooth\", \"easy\", \"secure\", \"reliable\", \"thank\", \"appreciate\", \"convenient\", \"peace\"}\n",
    "NEG_WORDS = {\"bad\", \"complain\", \"poor\", \"angry\", \"fraud\", \"delay\", \"crash\", \"issue\", \"problem\", \"expensive\", \"fee\", \"fees\", \"late\", \"refuse\", \"rejected\", \"unacceptable\", \"exorbitant\"}\n",
    "\n",
    "EPS = 1e-9\n",
    "TOP_JOB_K = 7  # how many top jobs to create one-hot for\n",
    "\n",
    "# LLM sentiment extraction\n",
    "LLM_BATCH_SIZE = 100\n",
    "LLM_MODEL = 'gemini-2.0-flash'\n",
    "\n",
    "# Simple mapping of job keywords to coarse job categories (reduces cardinality)\n",
    "JOB_KEYWORDS = {\n",
    "    'health': ['nurse', 'physic', 'clinic', 'health', 'doctor', 'therapist', 'pharm', 'care', 'hospital'],\n",
    "    'research': ['research', 'analyst', 'scientist', 'academic', 'lecturer'],\n",
    "    'education': ['teacher', 'professor', 'educat', 'tutor', 'school'],\n",
    "    'finance': ['bank', 'finance', 'account', 'auditor', 'financial', 'investment', 'insur'],\n",
    "    'it': ['developer', 'engineer', 'software', 'program', 'it ', 'data ', 'analytics', 'tech', 'system'],\n",
    "    'service': ['server', 'barista', 'waiter', 'retail', 'customer service', 'service'],\n",
    "    'gov': ['government', 'public', 'civil'],\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Helper functions\n",
    "# -----------------------------\n",
    "\n",
    "def normalize_job(job_str):\n",
    "    if not isinstance(job_str, str) or job_str.strip().lower() in {'none', 'missing'}:\n",
    "        return 'missing'\n",
    "    js = job_str.strip().lower()\n",
    "    # remove punctuation and common separators\n",
    "    js = js.replace('/', ' ').replace(',', ' ').replace('-', ' ')\n",
    "    # map to keyword categories\n",
    "    for cat, keywords in JOB_KEYWORDS.items():\n",
    "        for kw in keywords:\n",
    "            if kw in js:\n",
    "                return cat\n",
    "    # fallback: collapse repetitive whitespace\n",
    "    return 'other_' + '_'.join(js.split()[:3])  # keep up to first 3 tokens to reduce cardinality\n",
    "\n",
    "\n",
    "def safe_to_float(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "# Lightweight LLM sentiment helper\n",
    "def llm_feedback_sentiment(texts):\n",
    "    \"\"\"\n",
    "    Return list of (label, score) for each input text.\n",
    "    - label: 'positive'|'neutral'|'negative'\n",
    "    - score: float between -1.0 and 1.0\n",
    "    Uses the GenAI client in batches. LLM is always called for every batch; lexical fallback is only used for individual LLM failures within a batch.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    # lexical fallback local helper (only for individual LLM failures)\n",
    "    def _lexical(t):\n",
    "        t_l = (t or \"\").lower()\n",
    "        pos_cnt = sum(1 for w in t_l.split() if any(p == w or w.startswith(p) for p in POS_WORDS))\n",
    "        neg_cnt = sum(1 for w in t_l.split() if any(n == w or w.startswith(n) for n in NEG_WORDS))\n",
    "        polarity = pos_cnt - neg_cnt\n",
    "        if polarity > 0:\n",
    "            label = 'positive'; score = min(1.0, polarity / 5.0)\n",
    "        elif polarity < 0:\n",
    "            label = 'negative'; score = max(-1.0, polarity / 5.0)\n",
    "        else:\n",
    "            label = 'neutral'; score = 0.0\n",
    "        return (label, float(score))\n",
    "\n",
    "    # Batch request to the LLM\n",
    "    for i in range(0, len(texts), LLM_BATCH_SIZE):\n",
    "        batch = texts[i:i+LLM_BATCH_SIZE]\n",
    "        print(f\"Running LLM sentiment for records {i+1} to {i+len(batch)}...\")\n",
    "        prompt = (\n",
    "            \"You are a sentiment annotator. For each input sentence return a JSON array of objects with fields \"\n",
    "            '\"label\" and \"score\". Label MUST be one of: positive, neutral, negative. '\n",
    "            'Score MUST be a float between -1.0 and 1.0 (negative -> positive sentiment). '\n",
    "            'Return ONLY a single JSON array (no extra text).\\n\\n'\n",
    "            \"Inputs:\\n\"\n",
    "        )\n",
    "        for t in batch:\n",
    "            prompt += f\"- {t}\\n\"\n",
    "        try:\n",
    "            resp = client.models.generate_content(model=LLM_MODEL, contents=[prompt])\n",
    "            raw = resp.text if hasattr(resp, 'text') else str(resp)\n",
    "            m = re.search(r\"\\[.*?\\]\", raw, re.S)\n",
    "            payload = m.group(0) if m else raw\n",
    "            parsed = json.loads(payload)\n",
    "            # ensure parsed is a list of dict-like objects\n",
    "            for obj in parsed:\n",
    "                if not isinstance(obj, dict):\n",
    "                    out.append(_lexical(\"\"))\n",
    "                    continue\n",
    "                lab = obj.get('label', 'neutral')\n",
    "                sc = obj.get('score', 0.0)\n",
    "                # Normalize label and score\n",
    "                lab = str(lab).lower()\n",
    "                if lab not in {'positive', 'neutral', 'negative'}:\n",
    "                    lab = 'neutral'\n",
    "                try:\n",
    "                    scf = float(sc)\n",
    "                except Exception:\n",
    "                    scf = 0.0\n",
    "                scf = max(-1.0, min(1.0, scf))\n",
    "                out.append((lab, scf))\n",
    "        except Exception as e:\n",
    "            print(f\"LLM batch failed for records {i+1} to {i+len(batch)}: {e}\")\n",
    "            # On any LLM error, fallback to lexical for this batch\n",
    "            for t in batch:\n",
    "                out.append(_lexical(t))\n",
    "\n",
    "    # ensure length matches input\n",
    "    if len(out) < len(texts):\n",
    "        out.extend([('neutral', 0.0)] * (len(texts) - len(out)))\n",
    "    return out\n"
   ],
   "id": "bf3b10fa73c21dc1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Feature engineering function\n",
    "# -----------------------------\n",
    "\n",
    "def feature_enginner(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    # Standardize a few columns\n",
    "    if 'Gender' in df.columns:\n",
    "        df['Gender'] = df['Gender'].astype(str).str.upper().map({'M':'M','F':'F','': 'U'}).fillna('U')\n",
    "    if 'Persona' in df.columns:\n",
    "        df['Persona'] = df['Persona'].astype(str).fillna('missing')\n",
    "\n",
    "    # Numeric coercion\n",
    "    for c in ['Age','Annual_Income','Credit_Score','Account_Balance','Spending_Score','Loan_Amount','Loan_Term_Months','Loan_Default']:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "    # Missing indicators + simple imputations\n",
    "    for c in ['Age','Annual_Income','Credit_Score','Account_Balance','Spending_Score','Loan_Amount','Loan_Term_Months']:\n",
    "        if c in df.columns:\n",
    "            df[f'{c}_missing'] = df[c].isna().astype(int)\n",
    "            med = df[c].median()\n",
    "            df[c] = df[c].fillna(med)\n",
    "\n",
    "    for c in ['Gender','Persona','Job','Customer_Feedback']:\n",
    "        if c in df.columns:\n",
    "            df[f'{c}_missing'] = df[c].isna().astype(int)\n",
    "            df[c] = df[c].fillna('Missing').astype(str)\n",
    "\n",
    "    # Normalize Job into coarse categories to reduce cardinality\n",
    "    if 'Job' in df.columns:\n",
    "        df['Job_norm'] = df['Job'].apply(normalize_job)\n",
    "\n",
    "    # Clip extreme outliers by percentile and keep raw copies\n",
    "    clip_cols = ['Annual_Income','Account_Balance','Loan_Amount','Spending_Score','Credit_Score','Age']\n",
    "    for c in clip_cols:\n",
    "        if c in df.columns:\n",
    "            low = df[c].quantile(0.01)\n",
    "            high = df[c].quantile(0.99)\n",
    "            df[f'{c}_raw'] = df[c]\n",
    "            df[c] = df[c].clip(lower=low, upper=high)\n",
    "\n",
    "    # Log transforms for skewed monetary features\n",
    "    for c in ['Annual_Income','Account_Balance','Loan_Amount']:\n",
    "        if c in df.columns:\n",
    "            df[f'{c}_log1p'] = np.log1p(df[c].clip(lower=0))\n",
    "\n",
    "    # Derived features (ratios and interactions)\n",
    "    if {'Loan_Amount','Annual_Income'}.issubset(df.columns):\n",
    "        df['loan_to_income'] = df['Loan_Amount'] / (df['Annual_Income'] + EPS)\n",
    "    if {'Loan_Amount','Account_Balance'}.issubset(df.columns):\n",
    "        df['loan_to_balance'] = df['Loan_Amount'] / (df['Account_Balance'] + EPS)\n",
    "    if {'Account_Balance','Annual_Income'}.issubset(df.columns):\n",
    "        df['balance_to_income'] = df['Account_Balance'] / (df['Annual_Income'] + EPS)\n",
    "    if {'Loan_Amount','Loan_Term_Months'}.issubset(df.columns):\n",
    "        df['monthly_loan_payment_est'] = df['Loan_Amount'] / (df['Loan_Term_Months'] + EPS)\n",
    "    if 'Age' in df.columns and 'Annual_Income' in df.columns:\n",
    "        df['income_per_age'] = df['Annual_Income'] / (df['Age'] + EPS)\n",
    "\n",
    "    # Credit score buckets\n",
    "    if 'Credit_Score' in df.columns:\n",
    "        def credit_bucket(score):\n",
    "            try:\n",
    "                s = float(score)\n",
    "            except Exception:\n",
    "                return 'unknown'\n",
    "            if s < 580: return 'low'\n",
    "            if s < 670: return 'medium'\n",
    "            if s < 740: return 'good'\n",
    "            return 'excellent'\n",
    "        df['credit_bucket'] = df['Credit_Score'].apply(credit_bucket)\n",
    "\n",
    "    # Age groups\n",
    "    if 'Age' in df.columns:\n",
    "        def age_group(a):\n",
    "            if a < 25: return 'young'\n",
    "            if a < 45: return 'adult'\n",
    "            if a < 65: return 'mid'\n",
    "            return 'senior'\n",
    "        df['age_group'] = df['Age'].apply(age_group)\n",
    "\n",
    "    # Text / NLP features from Customer_Feedback (lexical)\n",
    "    def feedback_lexical_features(text: str):\n",
    "        if not isinstance(text, str):\n",
    "            text = ''\n",
    "        text_l = text.lower()\n",
    "        chars = len(text)\n",
    "        words = text_l.split()\n",
    "        wc = len(words)\n",
    "        avg_wlen = float(np.mean([len(w) for w in words])) if words else 0.0\n",
    "        pos_cnt = sum(1 for w in words if any(p == w or w.startswith(p) for p in POS_WORDS))\n",
    "        neg_cnt = sum(1 for w in words if any(n == w or w.startswith(n) for n in NEG_WORDS))\n",
    "        polarity = pos_cnt - neg_cnt\n",
    "        qmark = int('?' in text)\n",
    "        exclaim = int('!' in text)\n",
    "        capital_words = sum(1 for w in text.split() if w.isupper() and len(w) > 1)\n",
    "        has_url = int('http' in text.lower())\n",
    "        return {\n",
    "            'feedback_char_count': chars,\n",
    "            'feedback_word_count': wc,\n",
    "            'feedback_avg_word_len': avg_wlen,\n",
    "            'feedback_pos_count': pos_cnt,\n",
    "            'feedback_neg_count': neg_cnt,\n",
    "            'feedback_polarity': polarity,\n",
    "            'feedback_has_question': qmark,\n",
    "            'feedback_has_exclaim': exclaim,\n",
    "            'feedback_capital_words': capital_words,\n",
    "            'feedback_has_url': has_url,\n",
    "        }\n",
    "\n",
    "    if 'Customer_Feedback' in df.columns:\n",
    "        lex = df['Customer_Feedback'].apply(feedback_lexical_features).apply(pd.Series)\n",
    "        df = pd.concat([df, lex], axis=1)\n",
    "\n",
    "        # LLM-driven sentiment\n",
    "        try:\n",
    "            texts = df['Customer_Feedback'].fillna('').astype(str).tolist()\n",
    "            llm_out = llm_feedback_sentiment(texts)\n",
    "            df['feedback_llm_label'] = [lab for lab, sc in llm_out]\n",
    "            df['feedback_llm_score'] = [sc for lab, sc in llm_out]\n",
    "        except Exception as e:\n",
    "            # worst-case fallback: neutral for all\n",
    "            df['feedback_llm_label'] = ['neutral'] * len(df)\n",
    "            df['feedback_llm_score'] = [0.0] * len(df)\n",
    "            print('LLM sentiment failed, used neutral fallback:', e)\n",
    "\n",
    "    # Encode small categoricals (one-hot) and Job_norm via frequency/top-K\n",
    "    for c in ['Gender','Persona','credit_bucket','age_group']:\n",
    "        if c in df.columns:\n",
    "            dummies = pd.get_dummies(df[c], prefix=c)\n",
    "            df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "    if 'Job_norm' in df.columns:\n",
    "        counts = df['Job_norm'].value_counts()\n",
    "        topk = counts.head(TOP_JOB_K).index.tolist()\n",
    "        df['job_norm_freq'] = df['Job_norm'].map(counts).fillna(0)\n",
    "        for i, j in enumerate(topk, start=1):\n",
    "            df[f'job_norm_top_{i}'] = np.where(df['Job_norm'] == j, 1, 0)\n",
    "        df['job_norm_top_other'] = np.where(~df['Job_norm'].isin(topk), 1, 0)\n",
    "\n",
    "    # Final numeric coercions\n",
    "    final_numeric = ['Age','Annual_Income','Credit_Score','Account_Balance','Spending_Score','Loan_Amount','Loan_Term_Months','Loan_Default',\n",
    "                     'loan_to_income','loan_to_balance','balance_to_income','monthly_loan_payment_est','income_per_age']\n",
    "    for c in final_numeric:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0)\n",
    "\n",
    "    # Keep a short list of columns recommended for modeling\n",
    "    recommended_features = [c for c in df.columns if any(x in c for x in ['loan_to_','credit_','age_','feedback_','job_','Annual_Income','Account_Balance','Loan_Amount','Credit_Score'])]\n",
    "    df.attrs['recommended_features_sample'] = recommended_features[:60]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Run refined engineering and quick validations\n",
    "# -----------------------------\n",
    "print('Raw dataset shape:', df_customers.shape)\n",
    "df_clean = feature_enginner(df_customers)\n",
    "print('Cleaned dataset shape:', df_clean.shape)\n",
    "\n",
    "# Basic validations\n",
    "print('\\nCustomer_ID unique:', df_clean['Customer_ID'].nunique() == df_clean.shape[0])\n",
    "print('Credit bucket counts:\\n', df_clean['credit_bucket'].value_counts(dropna=False))\n",
    "print('\\nSample feedback features (first 5 rows):')\n",
    "print(df_clean[['Customer_ID','Customer_Feedback','feedback_word_count','feedback_polarity','feedback_has_question','feedback_has_exclaim']].head())\n",
    "\n",
    "# Save cleaned dataset for reproducibility\n",
    "try:\n",
    "    df_clean.to_csv('financial_data_cleaned.csv', index=False)\n",
    "    print('\\nSaved cleaned dataset to financial_data_cleaned.csv')\n",
    "except Exception as e:\n",
    "    print('Could not save cleaned CSV:', e)\n"
   ],
   "id": "b812791ea370d9ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_clean",
   "id": "ac8686b0f5b96e06"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Part 3: Exploratory Data Analysis (EDA)",
   "id": "d36eee1cae1eb77b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# For further step purposes, load df_clean instead of running all of the above parts again\n",
    "if os.path.exists('financial_data_cleaned.csv'):\n",
    "    df_clean = pd.read_csv('financial_data_cleaned.csv')\n",
    "    print('Loaded financial_data_cleaned.csv with shape', df_clean.shape)\n",
    "else:\n",
    "    print('financial_data_cleaned.csv not found, using in-memory df_clean if present.')"
   ],
   "id": "40fcea4b5237e4d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Shape and columns\n",
    "print('df_clean shape:', df_clean.shape)\n",
    "print('Columns:', df_clean.columns.tolist())\n",
    "\n",
    "# 2. Summary statistics for numeric columns\n",
    "print('\\nSummary statistics:')\n",
    "print(df_clean.describe().T)\n",
    "\n",
    "# 3. Value counts for key categoricals\n",
    "categorical_cols = ['Gender', 'Persona', 'credit_bucket', 'age_group']\n",
    "for col in categorical_cols:\n",
    "    if col in df_clean.columns:\n",
    "        print(f'\\nValue counts for {col}:')\n",
    "        print(df_clean[col].value_counts(dropna=False))\n",
    "\n",
    "# 4. Missing values\n",
    "print('\\nMissing values per column:')\n",
    "print(df_clean.isnull().sum())\n",
    "\n",
    "# 5. Sample rows\n",
    "print('\\nSample rows:')\n",
    "print(df_clean.head())\n",
    "\n",
    "# 6. Plot distributions for key features\n",
    "features_to_plot = ['Annual_Income', 'Credit_Score', 'Account_Balance', 'Loan_Amount', 'Loan_Default']\n",
    "for feat in features_to_plot:\n",
    "    if feat in df_clean.columns:\n",
    "        plt.figure(figsize=(6, 3))\n",
    "        if df_clean[feat].nunique() < 20:\n",
    "            sns.countplot(x=feat, data=df_clean)\n",
    "        else:\n",
    "            sns.histplot(df_clean[feat], kde=True, bins=30)\n",
    "        plt.title(f'Distribution of {feat}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# 7. Correlation heatmap (numeric features)\n",
    "numeric_cols = df_clean.select_dtypes(include=[float, int]).columns\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df_clean[numeric_cols].corr(), annot=False, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Heatmap (Numeric Features)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ],
   "id": "cf2b966d2b911374"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Predictive Model Development",
   "id": "6ca3402e0ac61878"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Part 4: Model Building\n",
    "\n",
    "## 4.1 ML Model utility methods for all models"
   ],
   "id": "d86c10df3d879190"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ],
   "id": "f0ee0b301fb30fb9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# LOAD & PREPARE DATA\n",
    "df = pd.read_csv('financial_data_cleaned.csv')\n",
    "\n",
    "# Define Target Variable\n",
    "target = 'Loan_Default'\n",
    "\n",
    "# Define Features (X) by dropping non-predictive or raw text columns\n",
    "# Drop original categorical columns if they have already been One-Hot Encoded\n",
    "cols_to_drop = [\n",
    "    'Loan_Default',      # The target itself\n",
    "    'Customer_ID',       # Unique ID, no predictive value\n",
    "    'Customer_Feedback', # Raw text (we use 'feedback_llm_score' instead)\n",
    "    'Job', 'Job_norm',   # High cardinality categorical text\n",
    "    'Gender', 'Persona', # Categorical (we use the encoded 'Gender_F', 'Persona_young_adult' etc.)\n",
    "    'credit_bucket', 'age_group', 'feedback_llm_label'\n",
    "]\n",
    "\n",
    "X = df.drop(columns=cols_to_drop, errors='ignore')\n",
    "y = df[target]\n",
    "\n",
    "# Ensure all boolean (True/False) columns are converted to 0/1 for the model\n",
    "bool_cols = X.select_dtypes(include=['bool']).columns\n",
    "X[bool_cols] = X[bool_cols].astype(int)"
   ],
   "id": "8e6458eb5d8d6022"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# SPLIT DATA (Train/Test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training on {X_train.shape[0]} rows, Testing on {X_test.shape[0]} rows.\")"
   ],
   "id": "35f1ddce387dc4fc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model 1 - Random Forest Classifier",
   "id": "be7834991d25203f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# BUILD & TRAIN MODEL\n",
    "\n",
    "# Initialize Random Forest with 100 trees\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
    "\n",
    "print(\"\\nTraining Random Forest Model...\")\n",
    "rf_model.fit(X_train, y_train)\n",
    "print(\"----Training Complete.----\")"
   ],
   "id": "61e3435fcca1b584"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# EVALUATION & METRICS\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# A. Accuracy Score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nModel Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# B. Detailed Classification Report (Precision, Recall, F1)\n",
    "print(\"\\n--- Classification Report ---\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# C. ROC and AUC Score\n",
    "if hasattr(rf_model, 'predict_proba'):\n",
    "    try:\n",
    "        print('ROC AUC:', roc_auc_score(y_test, rf_model.predict_proba(X_test)[:,1]))\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "# D. Confusion Matrix Visualization\n",
    "plt.figure(figsize=(6, 5))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix (Random Forest)')\n",
    "plt.xlabel('Predicted Label (0=Repaid, 1=Default)')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.show()\n"
   ],
   "id": "fbd1023692516f15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# FEATURE IMPORTANCE\n",
    "\n",
    "# Extracts the top 10 features that influenced the model the most\n",
    "importances = rf_model.feature_importances_\n",
    "feature_names = X.columns\n",
    "feat_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "feat_df = feat_df.sort_values(by='Importance', ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feat_df, palette='viridis')\n",
    "plt.title('Top 10 Drivers of Loan Default (Feature Importance)')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.show()"
   ],
   "id": "2bfbc95c92a26604"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# HYPERPARAMETER TUNING\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, 15, 20],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [2, 4, 6],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "# Stratified K-Fold Cross-Validation -> Splits data into 5 chunks, preserving the ratio of Defaulters in each chunk\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42), \n",
    "    param_grid, \n",
    "    cv=cv, \n",
    "    scoring='roc_auc', \n",
    "    n_jobs=-1,\n",
    "    verbose=1)\n",
    "\n",
    "print(\"Starting Hyperparameter Tuning ...\")\n",
    "\n",
    "rf_grid.fit(X_train, y_train)\n",
    "best_rf = rf_grid.best_estimator_\n",
    "\n",
    "print('\\nBest RF params:', rf_grid.best_params_)\n",
    "print(f\"\\nBest ROC-AUC Score (CV): {rf_grid.best_score_:.4f}\")"
   ],
   "id": "bbf160cb66134a9c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "# Analysis Model Complexity (Validation Curve)\n",
    "\n",
    "param_range = [2, 5, 8, 10, 12, 15, 20, 25, 30]\n",
    "\n",
    "train_scores, test_scores = validation_curve(\n",
    "    best_rf,\n",
    "    X_train, y_train,\n",
    "    param_name=\"max_depth\",\n",
    "    param_range=param_range,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(param_range, train_mean, label=\"Training Score\", color=\"darkorange\", marker='o')\n",
    "plt.plot(param_range, test_mean, label=\"Cross-Validation Score\", color=\"navy\", marker='o')\n",
    "\n",
    "# Mark the Optimal Point\n",
    "best_depth_idx = np.argmax(test_mean)\n",
    "best_depth = param_range[best_depth_idx]\n",
    "best_score = test_mean[best_depth_idx]\n",
    "\n",
    "plt.axvline(x=best_depth, color='green', linestyle='--', label=f'Optimal Depth ({best_depth})')\n",
    "plt.scatter(best_depth, best_score, s=100, c='green', marker='*') # Add a star!\n",
    "\n",
    "plt.title(\"Validation Curve: Impact of Tree Depth on Model Performance\")\n",
    "plt.xlabel(\"Model Complexity (Max Depth)\")\n",
    "plt.ylabel(\"ROC-AUC Score\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Interpretation:\\nThe validation curve confirms that a Max Depth of {best_depth} provides the best balance (ROC-AUC: {best_score:.3f}).\")"
   ],
   "id": "2ef8f370673dd258"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# EVALUATION ON TEST SET WITH BEST MODEL\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# A. Accuracy Score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nModel Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# B. Detailed Classification Report\n",
    "print('\\n--- Classification Report ---')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# C. ROC and AUC Score\n",
    "if hasattr(best_rf, 'predict_proba'):\n",
    "    try:\n",
    "        print('\\nROC AUC:', roc_auc_score(y_test, best_rf.predict_proba(X_test)[:,1]))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# D. Confusion Matrix Visualization\n",
    "plt.figure(figsize=(6, 5))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix (Random Forest)')\n",
    "plt.xlabel('Predicted Label (0=Repaid, 1=Default)')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.show()"
   ],
   "id": "baf3c6a73873d2c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model 2 - XGBoost",
   "id": "d62d5f11fdfb6e9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# BASELINE XGBOOST MODEL\n",
    "# Train a baseline XGBoost with default parameters to establish performance baseline\n",
    "\n",
    "xgb_baseline = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "\n",
    "print(\"\\nTraining Baseline XGBoost Model...\")\n",
    "xgb_baseline.fit(X_train, y_train)\n",
    "print(\"----Training Complete.----\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_baseline = xgb_baseline.predict(X_test)\n",
    "y_proba_baseline = xgb_baseline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluation metrics\n",
    "accuracy_baseline = accuracy_score(y_test, y_pred_baseline)\n",
    "roc_auc_baseline = roc_auc_score(y_test, y_proba_baseline)\n",
    "\n",
    "print(f\"\\nBaseline XGBoost Model Accuracy: {accuracy_baseline:.2%}\")\n",
    "print(\"\\n--- Classification Report ---\")\n",
    "print(classification_report(y_test, y_pred_baseline))\n",
    "print(f\"ROC AUC: {roc_auc_baseline:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "cm_baseline = confusion_matrix(y_test, y_pred_baseline)\n",
    "sns.heatmap(cm_baseline, annot=True, fmt='d', cmap='Oranges', cbar=False)\n",
    "plt.title('Confusion Matrix (Baseline XGBoost)')\n",
    "plt.xlabel('Predicted Label (0=Repaid, 1=Default)')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.show()\n",
    "\n",
    "# Feature Importance (top 10)\n",
    "importances_baseline = xgb_baseline.feature_importances_\n",
    "feat_df_baseline = pd.DataFrame({'Feature': X.columns, 'Importance': importances_baseline})\n",
    "feat_df_baseline = feat_df_baseline.sort_values(by='Importance', ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feat_df_baseline, palette='magma')\n",
    "plt.title('Top 10 Drivers of Loan Default (Baseline XGBoost)')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.show()"
   ],
   "id": "8662d9501e32a5f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# HYPERPARAMETER TUNING FOR XGBOOST\n",
    "\n",
    "# Expanded hyperparameters with Regularization to prevent overfitting\n",
    "xgb_params = {\n",
    "    'n_estimators': [300],\n",
    "    'max_depth': [1, 2, 3, 4, 5, 7, 10],\n",
    "    'learning_rate': [0.01, 0.05],\n",
    "    'subsample': [0.6, 0.8],\n",
    "    'colsample_bytree': [0.6,0.8],\n",
    "    'min_child_weight': [5, 10],\n",
    "    'gamma': [0.5, 1]\n",
    "}\n",
    "\n",
    "# Stratified K-Fold Cross-Validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "X_train_grid, X_val, y_train_grid, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.15, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "fit_params = {\n",
    "    'eval_set': [(X_val, y_val)],\n",
    "    'verbose': False\n",
    "}\n",
    "\n",
    "# Enable return_train_score to plot train vs test\n",
    "xgb_grid = GridSearchCV(\n",
    "    XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, early_stopping_rounds=50),\n",
    "    xgb_params,\n",
    "    cv=cv,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print('\\nStarting Hyperparameter Tuning for XGBoost...')\n",
    "xgb_grid.fit(X_train_grid, y_train_grid, **fit_params)\n",
    "best_xgb = xgb_grid.best_estimator_\n",
    "\n",
    "print('\\nBest XGBoost params:', xgb_grid.best_params_)\n",
    "print(f\"\\nBest ROC-AUC Score (CV): {xgb_grid.best_score_:.4f}\")\n",
    "\n",
    "# Extract CV results\n",
    "results_df = pd.DataFrame(xgb_grid.cv_results_)\n",
    "\n",
    "# Process results to get the best test and train score for each max_depth\n",
    "# We aggregate by max using the remaining best parameters for that depth\n",
    "depth_data = results_df.groupby('param_max_depth').agg({'mean_test_score': 'max', 'mean_train_score': 'max'}).reset_index()\n",
    "depth_data['param_max_depth'] = depth_data['param_max_depth'].astype(int)\n",
    "depth_data = depth_data.sort_values('param_max_depth')\n",
    "\n",
    "x = depth_data['param_max_depth'].values\n",
    "train_auc = depth_data['mean_train_score'].values\n",
    "test_auc = depth_data['mean_test_score'].values\n",
    "labels = x\n",
    "\n",
    "# Find indices\n",
    "best_idx = np.argmax(test_auc)\n",
    "# Overfitting: simplistic def -> max (train - test)\n",
    "overfit_idx = np.argmax(train_auc - test_auc)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, train_auc, label='Train AUC', marker='o')\n",
    "plt.plot(x, test_auc, label='Test AUC', marker='o')\n",
    "\n",
    "plt.axvline(x[best_idx], color='green', linestyle='--', label=f'Best test AUC: Depth {labels[best_idx]} ({test_auc[best_idx]:.4f})')\n",
    "plt.axvline(x[overfit_idx], color='red', linestyle=':', label=f'Largest gap: Depth {labels[overfit_idx]}')\n",
    "\n",
    "plt.xticks(x, labels)\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('ROC AUC')\n",
    "plt.title('XGBoost AUC vs max_depth (Reguralized)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "d2af8f54997d6ad5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluation using the best model\n",
    "try:\n",
    "    y_pred_xgb = best_xgb.predict(X_test)\n",
    "    y_proba_xgb = best_xgb.predict_proba(X_test)[:, 1]\n",
    "except Exception:\n",
    "    y_pred_xgb = best_xgb.predict(X_test)\n",
    "    y_proba_xgb = y_pred_xgb.astype(float)\n",
    "\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "try:\n",
    "    roc_auc_xgb = roc_auc_score(y_test, y_proba_xgb)\n",
    "except Exception:\n",
    "    roc_auc_xgb = None\n",
    "\n",
    "print(f\"\\nModel Accuracy: {accuracy_xgb:.2%}\")\n",
    "print('\\n--- Classification Report ---')\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "if roc_auc_xgb is not None:\n",
    "    print(f'ROC AUC: {roc_auc_xgb:.4f}')\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "cm = confusion_matrix(y_test, y_pred_xgb)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges', cbar=False)\n",
    "plt.title('Confusion Matrix (XGBoost)')\n",
    "plt.xlabel('Predicted Label (0=Repaid, 1=Default)')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.show()\n",
    "\n",
    "# ROC curve\n",
    "if roc_auc_xgb is not None:\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba_xgb)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(fpr, tpr, label=f'XGBoost ROC (AUC={roc_auc_xgb:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve (XGBoost)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Precision-Recall\n",
    "try:\n",
    "    prec, recall, _ = precision_recall_curve(y_test, y_proba_xgb)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(recall, prec, label='XGBoost Precision-Recall')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve (XGBoost)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Feature importance (top 10)\n",
    "try:\n",
    "    importances_xgb = best_xgb.feature_importances_\n",
    "    feature_names = X.columns\n",
    "    feat_df_xgb = pd.DataFrame({'Feature': feature_names, 'Importance': importances_xgb})\n",
    "    feat_df_xgb = feat_df_xgb.sort_values(by='Importance', ascending=False).head(10)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feat_df_xgb, palette='magma')\n",
    "    plt.title('Top 10 Drivers of Loan Default (XGBoost Feature Importance)')\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.show()\n",
    "except Exception:\n",
    "    print('Could not extract XGBoost feature importances.')\n"
   ],
   "id": "ffda1b473931aa96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# CHECK FOR OVERFITTING/UNDERFITTING (Learning Curves)\n",
    "# Re-train a fresh model using the best found parameters to track performance over iterations\n",
    "\n",
    "best_params = xgb_grid.best_params_\n",
    "\n",
    "eval_model = XGBClassifier(\n",
    "    **best_params,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=[\"logloss\", \"auc\"], # Track both metrics\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "\n",
    "print(\"Training evaluation model to generate learning curves...\")\n",
    "eval_model.fit(X_train, y_train, eval_set=eval_set, verbose=False)\n",
    "\n",
    "results = eval_model.evals_result()\n",
    "epochs = len(results['validation_0']['logloss'])\n",
    "x_axis = range(0, epochs)\n",
    "\n",
    "# Plot Log Loss and AUC\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Log Loss\n",
    "ax[0].plot(x_axis, results['validation_0']['logloss'], label='Train')\n",
    "ax[0].plot(x_axis, results['validation_1']['logloss'], label='Test')\n",
    "ax[0].legend()\n",
    "ax[0].set_ylabel('Log Loss')\n",
    "ax[0].set_title('XGBoost Log Loss')\n",
    "\n",
    "# AUC\n",
    "ax[1].plot(x_axis, results['validation_0']['auc'], label='Train')\n",
    "ax[1].plot(x_axis, results['validation_1']['auc'], label='Test')\n",
    "ax[1].legend()\n",
    "ax[1].set_ylabel('AUC')\n",
    "ax[1].set_title('XGBoost AUC Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "a9c01e3d890267cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model 3 - KNN",
   "id": "b2d4ab28a36ad661"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check for Underfit/Overfit using different K values\n",
    "k_values = range(1, 31)  # test K from 1 to 30\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('knn', KNeighborsClassifier(n_neighbors=k))\n",
    "    ])\n",
    "\n",
    "    knn_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    train_acc = knn_pipeline.score(X_train, y_train)\n",
    "    test_acc  = knn_pipeline.score(X_test, y_test)\n",
    "\n",
    "    train_scores.append(train_acc)\n",
    "    test_scores.append(test_acc)"
   ],
   "id": "4c10387c40d3a05e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plot Training vs Testing Accuracy\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(k_values, train_scores, label=\"Training Accuracy\", marker='o')\n",
    "plt.plot(k_values, test_scores, label=\"Testing Accuracy\", marker='o')\n",
    "plt.axvline(x=np.argmax(test_scores)+1, color='red', linestyle='--',\n",
    "            label=f\"Best K = {np.argmax(test_scores)+1}\")\n",
    "\n",
    "\n",
    "plt.title(\"KNN Model Fit Diagnostic  Detect Underfitting vs Overfitting\")\n",
    "plt.xlabel(\"Number of Neighbors (K)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest K = {np.argmax(test_scores)+1} (Highest Test Accuracy {max(test_scores):.2%})\")\n"
   ],
   "id": "c7331049492134fa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "KNN model complexity is changed by adjusting the number of neighbors (K=1 to K=30).\n",
    "The experiment successfully demonstrates three classical learning behaviours:\n",
    "\n",
    "- Overfitting was observed at low K (e.g., K=13), where training accuracy approached 100% while testing accuracy was significantly lower.\n",
    "\n",
    "- Optimal fitting occurred around K=7, where the model achieved the highest testing accuracy with a minimal train-test gap, indicating good generalization.\n",
    "\n",
    "- Underfitting appeared at high K values (e.g., K>20), where both training and testing accuracy decreased, implying overly smooth decision boundaries.\n",
    "\n",
    "These findings validate the robustness of the modelling pipeline, as the model reacts to complexity changes in a theoretically consistent manner."
   ],
   "id": "cac91535d9a47dbd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Train Final Model using Best K\n",
    "best_k = np.argmax(test_scores)+1\n",
    "knn_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=best_k))\n",
    "])\n",
    "\n",
    "print(\"\\nTraining Final KNN Model...\")\n",
    "knn_pipeline.fit(X_train, y_train)\n",
    "print(\"Training Complete.\")"
   ],
   "id": "28e937ab84d8ce60"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Model Evaluation\n",
    "y_pred = knn_pipeline.predict(X_test)\n",
    "y_proba = knn_pipeline.predict_proba(X_test)[:,1]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "print(f\"\\nFinal Model Accuracy: {accuracy:.2%}\")\n",
    "print(f\"ROC AUC: {auc:.4f}\")\n",
    "print(\"--- Classification Report ---\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d',\n",
    "            cmap='Greens', cbar=False)\n",
    "plt.title(f\"KNN Confusion Matrix (k={best_k})\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(fpr, tpr, label=f\"AUC={auc:.3f}\")\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve (KNN)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(rec, prec, label=\"Precision-Recall\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve (KNN)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Permutation Feature Importance\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "perm = permutation_importance(knn_pipeline, X_test, y_test,\n",
    "                              n_repeats=10, scoring='roc_auc', random_state=42)\n",
    "\n",
    "feat_df = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Importance\": perm.importances_mean,\n",
    "    \"Std\": perm.importances_std\n",
    "}).sort_values(by=\"Importance\", ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=\"Importance\", y=\"Feature\", data=feat_df, palette=\"coolwarm\")\n",
    "plt.title(\"Top 10 Feature Importance (KNN)\")\n",
    "plt.show()\n"
   ],
   "id": "a53ae3cebaf2d211"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model 4 - Base decision tree",
   "id": "e28252c68bc7ba13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Train Baseline Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dt.predict(X_test)\n",
    "y_proba = dt.predict_proba(X_test)[:, 1] if hasattr(dt, \"predict_proba\") else None\n",
    "\n",
    "print(\"Baseline Decision Tree\")\n",
    "print(\"Accuracy:\", f\"{accuracy_score(y_test, y_pred):.2%}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "if y_proba is not None:\n",
    "    try:\n",
    "        print(\"ROC AUC:\", f\"{roc_auc_score(y_test, y_proba):.4f}\")\n",
    "    except Exception:\n",
    "        pass"
   ],
   "id": "c161341de892d2a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.title(\"Decision Tree - Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "# Feature importances (top 10)\n",
    "fi = pd.Series(dt.feature_importances_, index=X_train.columns).sort_values(ascending=False).head(10)\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.barplot(x=fi.values, y=fi.index, palette=\"viridis\")\n",
    "plt.title(\"Top 10 Feature Importances (Decision Tree)\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Small Grid Search for hyperparameters (fast)\n",
    "param_grid = {\n",
    "    \"max_depth\": [3, 5, 8, 12, None],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"criterion\": [\"gini\", \"entropy\"]\n",
    "}\n",
    "gs = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5, scoring=\"roc_auc\", n_jobs=-1, verbose=1)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "best_dt = gs.best_estimator_\n",
    "print(\"\\nGridSearch Best Params:\", gs.best_params_)\n",
    "y_pred_best = best_dt.predict(X_test)\n",
    "y_proba_best = best_dt.predict_proba(X_test)[:, 1] if hasattr(best_dt, \"predict_proba\") else None\n",
    "\n",
    "print(\"Best Decision Tree\")\n",
    "print(\"Accuracy:\", f\"{accuracy_score(y_test, y_pred_best):.4f}\")\n",
    "print(classification_report(y_test, y_pred_best))\n",
    "if y_proba_best is not None:\n",
    "    try:\n",
    "        print(\"ROC AUC:\", f\"{roc_auc_score(y_test, y_proba_best):.4f}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Plot a shallow visualization of the best tree for interpretability\n",
    "plt.figure(figsize=(14,6))\n",
    "plot_tree(best_dt, feature_names=X_train.columns, class_names=[\"repaid\",\"default\"], filled=True, max_depth=3, fontsize=8)\n",
    "plt.title(\"Decision Tree (top 3 levels) - Best Estimator\")\n",
    "plt.show()"
   ],
   "id": "c11791828a4d3d7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Decision Tree complexity sweep (max_depth) to demonstrate underfit/best/overfit behaviour\n",
    "depths = [1, 2, 3, 4, 5, 8, 12, 20, None]\n",
    "train_auc = []\n",
    "test_auc = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "labels = [str(d) for d in depths]\n",
    "for d in depths:\n",
    "    clf = DecisionTreeClassifier(max_depth=d, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    # probabilities fallback to predictions if not available\n",
    "    try:\n",
    "        y_tr_proba = clf.predict_proba(X_train)[:, 1]\n",
    "        y_te_proba = clf.predict_proba(X_test)[:, 1]\n",
    "    except Exception:\n",
    "        y_tr_proba = clf.predict(X_train).astype(float)\n",
    "        y_te_proba = clf.predict(X_test).astype(float)\n",
    "    train_auc.append(roc_auc_score(y_train, y_tr_proba))\n",
    "    test_auc.append(roc_auc_score(y_test, y_te_proba))\n",
    "    train_acc.append(accuracy_score(y_train, clf.predict(X_train)))\n",
    "    test_acc.append(accuracy_score(y_test, clf.predict(X_test)))\n",
    "\n",
    "# Convert to arrays for easy computation\n",
    "import numpy as _np\n",
    "train_auc = _np.array(train_auc)\n",
    "test_auc = _np.array(test_auc)\n",
    "train_acc = _np.array(train_acc)\n",
    "test_acc = _np.array(test_acc)\n",
    "\n",
    "# Locate best (highest test AUC), most-overfitting (largest train-test AUC gap), and most-underfitting (lowest test AUC)\n",
    "best_idx = int(_np.nanargmax(test_auc))\n",
    "gap = train_auc - test_auc\n",
    "overfit_idx = int(_np.nanargmax(gap))\n",
    "underfit_idx = int(_np.nanargmin(test_auc))\n",
    "\n",
    "# Plot AUC and Accuracy vs max_depth\n",
    "x = range(len(depths))\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "# AUC\n",
    "axes[0].plot(x, train_auc, label='Train AUC', marker='o')\n",
    "axes[0].plot(x, test_auc, label='Test AUC', marker='o')\n",
    "axes[0].axvline(best_idx, color='green', linestyle='--', label=f'Best test AUC: {labels[best_idx]}')\n",
    "axes[0].axvline(overfit_idx, color='red', linestyle=':', label=f'Largest gap: {labels[overfit_idx]}')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(labels)\n",
    "axes[0].set_xlabel('max_depth')\n",
    "axes[0].set_ylabel('ROC AUC')\n",
    "axes[0].set_title('Decision Tree AUC vs max_depth')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "# Accuracy\n",
    "axes[1].plot(x, train_acc, label='Train Accuracy', marker='o')\n",
    "axes[1].plot(x, test_acc, label='Test Accuracy', marker='o')\n",
    "axes[1].axvline(best_idx, color='green', linestyle='--', label=f'Best test AUC: {labels[best_idx]}')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(labels)\n",
    "axes[1].set_xlabel('max_depth')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Decision Tree Accuracy vs max_depth')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# If the current best_dt has a different max_depth, show comparison\n",
    "if 'best_dt' in globals():\n",
    "    bd = getattr(best_dt, 'max_depth', None)\n",
    "    print(f' Current GridSearch best_dt.max_depth = {bd} (type: {type(bd).__name__})')\n",
    "    # Compare test AUC\n",
    "    try:\n",
    "        if bd in depths:\n",
    "            idx = labels.index(str(bd))\n",
    "            print(f' test AUC at best_dt.max_depth: {test_auc[idx]:.4f}')\n",
    "        else:\n",
    "            print(' best_dt.max_depth not in sweep depths; consider enlarging sweep to include None or larger depths')\n",
    "    except Exception:\n",
    "        pass"
   ],
   "id": "4494eed22faba92d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Decision Tree complexity sweep summary:** <br>\n",
    "Best test AUC at max_depth=3 -> Test AUC=0.7833, Train AUC=0.8517  <br>\n",
    "\n",
    "Decision model complexity is changed by adjusting the depth (K=1 to K=20 and K=None).\n",
    "The experiment successfully demonstrates three classical learning behaviours: <br>\n",
    "\n",
    "- Underfitting occurs from K=1 to K=2 as both the train and test accuracy decreases.<br>\n",
    "- Overfitting occurs from K=4 to K=20 as the train accuracy gets to 100% but the test accuracy decreases over the depth. <br>\n",
    "- The optimum depth for the decision tree model is K=3."
   ],
   "id": "529fd8995d5b6f96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Assumes: best_dt, X_test, y_test (and optionally y_proba_best) are in the notebook namespace\n",
    "# Obtain predicted probabilities (fallback to model if y_proba_best not present)\n",
    "try:\n",
    "    y_proba_dt = y_proba_best\n",
    "except NameError:\n",
    "    if hasattr(best_dt, \"predict_proba\"):\n",
    "        y_proba_dt = best_dt.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_proba_dt = best_dt.predict(X_test).astype(float)\n",
    "\n",
    "# Compute ROC and PR metrics\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba_dt)\n",
    "roc_auc = roc_auc_score(y_test, y_proba_dt)\n",
    "\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba_dt)\n",
    "ap = np.trapz(prec, rec)  # approximate area under PR curve\n",
    "\n",
    "# Plot side-by-side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# ROC\n",
    "axes[0].plot(fpr, tpr, lw=2, label=f'AUC = {roc_auc:.3f}')\n",
    "axes[0].plot([0, 1], [0, 1], color='k', linestyle='--', lw=1)\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('ROC Curve (Decision Tree)')\n",
    "axes[0].legend(loc='lower right')\n",
    "\n",
    "# Precision-Recall\n",
    "axes[1].plot(rec, prec, lw=2, label=\"Decision Tree Precision-Recall\")\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Precision-Recall Curve (Decision Tree)')\n",
    "axes[1].legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "feed1ab0525076ec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model 5 - Logistic Regression",
   "id": "27c44658d1448f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check for Underfit/Overfit using different C values (regularization strength)\n",
    "# C is the inverse of regularization: smaller C = stronger regularization (simpler model)\n",
    "# Logarithmic scale from 0.001 to 100\n",
    "c_values = [0.001, 0.01, 0.1, 0.5, 1, 2, 5, 10, 20, 50, 100]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for c in c_values:\n",
    "    lr_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('lr', LogisticRegression(C=c, max_iter=1000, random_state=42, solver='lbfgs'))\n",
    "    ])\n",
    "\n",
    "    lr_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    train_acc = lr_pipeline.score(X_train, y_train)\n",
    "    test_acc  = lr_pipeline.score(X_test, y_test)\n",
    "\n",
    "    train_scores.append(train_acc)\n",
    "    test_scores.append(test_acc)\n",
    "\n",
    "print(\"C values tested:\", c_values)\n",
    "print(\"Training scores:\", [f\"{s:.4f}\" for s in train_scores])\n",
    "print(\"Testing scores:\", [f\"{s:.4f}\" for s in test_scores])"
   ],
   "id": "741558706e3873c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plot Training vs Testing Accuracy\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(range(len(c_values)), train_scores, label=\"Training Accuracy\", marker='o')\n",
    "plt.plot(range(len(c_values)), test_scores, label=\"Testing Accuracy\", marker='o')\n",
    "\n",
    "best_idx = np.argmax(test_scores)\n",
    "plt.axvline(x=best_idx, color='red', linestyle='--',\n",
    "            label=f\"Best C = {c_values[best_idx]}\")\n",
    "\n",
    "plt.xticks(range(len(c_values)), [f\"{c}\" for c in c_values], rotation=45)\n",
    "plt.title(\"Logistic Regression Model Fit Diagnostic  Detect Underfitting vs Overfitting\")\n",
    "plt.xlabel(\"Regularization Parameter C (inverse of regularization strength)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best C = {c_values[best_idx]} (Highest Test Accuracy {max(test_scores):.2%})\")"
   ],
   "id": "f568dc3423aef462"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Train Final Model using Best C\n",
    "best_c = c_values[best_idx]\n",
    "lr_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lr', LogisticRegression(C=best_c, max_iter=1000, random_state=42, solver='lbfgs'))\n",
    "])\n",
    "\n",
    "print(\"\\nTraining Final Logistic Regression Model...\")\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "print(\"Training Complete.\")"
   ],
   "id": "aab86a8673e3a652"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Model Evaluation\n",
    "y_pred = lr_pipeline.predict(X_test)\n",
    "y_proba = lr_pipeline.predict_proba(X_test)[:,1]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "print(f\"\\nFinal Model Accuracy: {accuracy:.2%}\")\n",
    "print(f\"ROC AUC: {auc:.4f}\\n\")\n",
    "print(\"--- Classification Report ---\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d',\n",
    "            cmap='Purples', cbar=False)\n",
    "plt.title(f\"Logistic Regression Confusion Matrix (C={best_c})\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(fpr, tpr, label=f\"AUC={auc:.3f}\")\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve (Logistic Regression)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(rec, prec, label=\"Precision-Recall\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve (Logistic Regression)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Coefficient-based Feature Importance\n",
    "# Extract coefficients from the logistic regression model\n",
    "lr_model = lr_pipeline.named_steps['lr']\n",
    "coefficients = lr_model.coef_[0]  # Get coefficients for the positive class\n",
    "\n",
    "# Use absolute values for importance (magnitude of impact)\n",
    "feat_df = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Coefficient\": coefficients,\n",
    "    \"Abs_Coefficient\": np.abs(coefficients)\n",
    "}).sort_values(by=\"Abs_Coefficient\", ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "colors = ['green' if c > 0 else 'red' for c in feat_df['Coefficient']]\n",
    "sns.barplot(x=\"Abs_Coefficient\", y=\"Feature\", data=feat_df, palette=colors)\n",
    "plt.title(\"Top 10 Feature Importance (Logistic Regression - Coefficient Magnitude)\")\n",
    "plt.xlabel(\"Absolute Coefficient Value\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feat_df[['Feature', 'Coefficient']].to_string(index=False))\n",
    "print(\"\\nNote: Green bars = positive coefficient (increases default probability)\")\n",
    "print(\"      Red bars = negative coefficient (decreases default probability)\")\n",
    "\n",
    "\n"
   ],
   "id": "b26f7bdff88832ed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Part 5: Model Evaluation",
   "id": "edf60bfbbee80e28"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Side by side comparison for each models in terms of accuracy, precision, recall, f1-score, ROC-AUC values and ROC-AUC curves",
   "id": "33f52ee71a650f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Collect metrics for all models\n",
    "models_metrics = {}\n",
    "\n",
    "# Model 1: Random Forest (best_rf)\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "y_proba_rf = best_rf.predict_proba(X_test)[:, 1]\n",
    "models_metrics['Random Forest'] = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_rf),\n",
    "    'precision': precision_score(y_test, y_pred_rf),\n",
    "    'recall': recall_score(y_test, y_pred_rf),\n",
    "    'f1': f1_score(y_test, y_pred_rf),\n",
    "    'roc_auc': roc_auc_score(y_test, y_proba_rf),\n",
    "    'fpr': roc_curve(y_test, y_proba_rf)[0],\n",
    "    'tpr': roc_curve(y_test, y_proba_rf)[1]\n",
    "}\n",
    "\n",
    "# Model 2: XGBoost\n",
    "models_metrics['XGBoost'] = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_xgb),\n",
    "    'precision': precision_score(y_test, y_pred_xgb),\n",
    "    'recall': recall_score(y_test, y_pred_xgb),\n",
    "    'f1': f1_score(y_test, y_pred_xgb),\n",
    "    'roc_auc': roc_auc_xgb,\n",
    "    'fpr': roc_curve(y_test, y_proba_xgb)[0],\n",
    "    'tpr': roc_curve(y_test, y_proba_xgb)[1]\n",
    "}\n",
    "\n",
    "# Model 3: KNN\n",
    "y_pred_knn = knn_pipeline.predict(X_test)\n",
    "y_proba_knn = knn_pipeline.predict_proba(X_test)[:, 1]\n",
    "models_metrics['KNN'] = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_knn),\n",
    "    'precision': precision_score(y_test, y_pred_knn),\n",
    "    'recall': recall_score(y_test, y_pred_knn),\n",
    "    'f1': f1_score(y_test, y_pred_knn),\n",
    "    'roc_auc': roc_auc_score(y_test, y_proba_knn),\n",
    "    'fpr': roc_curve(y_test, y_proba_knn)[0],\n",
    "    'tpr': roc_curve(y_test, y_proba_knn)[1]\n",
    "}\n",
    "\n",
    "# Model 4: Decision Tree\n",
    "models_metrics['Decision Tree'] = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_best),\n",
    "    'precision': precision_score(y_test, y_pred_best),\n",
    "    'recall': recall_score(y_test, y_pred_best),\n",
    "    'f1': f1_score(y_test, y_pred_best),\n",
    "    'roc_auc': roc_auc_score(y_test, y_proba_dt),\n",
    "    'fpr': roc_curve(y_test, y_proba_dt)[0],\n",
    "    'tpr': roc_curve(y_test, y_proba_dt)[1]\n",
    "}\n",
    "\n",
    "# Model 5: Logistic Regression\n",
    "y_pred_lr = lr_pipeline.predict(X_test)\n",
    "y_proba_lr = lr_pipeline.predict_proba(X_test)[:, 1]\n",
    "models_metrics['Logistic Regression'] = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_lr),\n",
    "    'precision': precision_score(y_test, y_pred_lr),\n",
    "    'recall': recall_score(y_test, y_pred_lr),\n",
    "    'f1': f1_score(y_test, y_pred_lr),\n",
    "    'roc_auc': roc_auc_score(y_test, y_proba_lr),\n",
    "    'fpr': roc_curve(y_test, y_proba_lr)[0],\n",
    "    'tpr': roc_curve(y_test, y_proba_lr)[1]\n",
    "}\n",
    "\n",
    "model_names = list(models_metrics.keys())\n",
    "metrics_names = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "\n",
    "# Plot 1: Accuracy, Precision, Recall, F1-Score, ROC-AUC Comparison (Bar Chart)\n",
    "# Per-metric bar charts(one chart per metric)\n",
    "title_map = {'accuracy': 'Accuracy', 'precision': 'Precision', 'recall': 'Recall', 'f1': 'F1-Score', 'roc_auc': 'ROC-AUC'}\n",
    "\n",
    "for metric in metrics_names:\n",
    "    values = [models_metrics[model][metric] for model in model_names]\n",
    "    x = np.arange(len(model_names))\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    bars = ax.bar(x, values, width=0.6, color=plt.cm.Set2(np.linspace(0, 1, len(model_names))))\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(model_names, rotation=35, ha='right')\n",
    "    ax.set_ylabel('Score', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'Model Comparison: {title_map.get(metric, metric)}', fontsize=13, fontweight='bold')\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.grid(axis='y', alpha=0.25)\n",
    "\n",
    "    # Annotate bar values\n",
    "    for bar in bars:\n",
    "        h = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, h + 0.01, f'{h:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot 2: ROC-AUC Curve Comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "for model in model_names:\n",
    "    fpr = models_metrics[model]['fpr']\n",
    "    tpr = models_metrics[model]['tpr']\n",
    "    auc = models_metrics[model]['roc_auc']\n",
    "    ax.plot(fpr, tpr, linewidth=2, label=f'{model} (AUC={auc:.3f})')\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "ax.set_title('ROC Curve Comparison - All Models', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='lower right', fontsize=10)\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print metrics summary table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "metrics_df = pd.DataFrame(models_metrics).T\n",
    "print(metrics_df[['accuracy', 'precision', 'recall', 'f1', 'roc_auc']].round(4))\n",
    "print(\"=\"*80)"
   ],
   "id": "a3761f1afdb13b6f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
